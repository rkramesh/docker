#!/bin/bash

# === Improved Photo Sync Script ===
# Features: HEIC conversion, chronological upload, state management, resume capability

# ===========================================================================================
# ü§ñ LLM AGENT INSTRUCTIONS - READ FIRST BEFORE ANY MODIFICATIONS
# ===========================================================================================
#
# üèóÔ∏è INFRASTRUCTURE CONSTRAINTS (DO NOT VIOLATE):
#   Platform: Docker + Raspberry Pi 3 (ARM64, 4 cores, 1GB RAM, SD card)
#   Storage:  /scripts/* = PERSISTENT ‚úÖ | /tmp/* = EPHEMERAL ‚ùå (wiped on restart)
#   Network:  Unstable WiFi, implement retries + timeouts
#   CPU:      MAX 3 parallel jobs (PARALLEL_JOBS=3) to avoid overwhelming RPi3
#
# üéØ CORE DESIGN PATTERNS (ALREADY OPTIMIZED):
#   ‚úÖ Hash caching (O(1) lookups): SENT_CACHE, CONVERT_CACHE
#   ‚úÖ Persistent state tracking: All .txt files in /scripts/state/
#   ‚úÖ Separate test state: _test suffix for safe testing without affecting production
#   ‚úÖ Resume capability: Container restart = perfect resume from last state
#   ‚úÖ Mixed media batching: Photos + small videos together
#   ‚úÖ Three-phase pipeline: Convert ‚Üí Manifest ‚Üí Upload
#   ‚úÖ macOS/Linux compatibility: date handling, stat commands
#
# ‚ö†Ô∏è  ANTI-PATTERNS TO AVOID:
#   ‚ùå State files in /tmp (lost on container restart)
#   ‚ùå GNU parallel with complex parameter passing (spaces in names break it)
#   ‚ùå Memory loading entire datasets (1GB RAM limit)
#   ‚ùå Missing error handling/retries
#   ‚ùå Hardcoded paths without env fallbacks
#   ‚ùå Breaking chronological order
#
# üîß PERFORMANCE CRITICAL SECTIONS:
#   ‚Ä¢ load_tracking_cache(): Loads files into hash tables for O(1) lookups
#   ‚Ä¢ Bash background jobs: HEIC conversion uses RPi-optimized concurrency
#   ‚Ä¢ date_to_timestamp(): Handles macOS/Linux differences for EXIF sorting
#   ‚Ä¢ Mixed batching: Photos+videos together, large videos via API server
#
# üìù MAINTENANCE HINTS:
#   ‚Ä¢ Update PARALLEL_JOBS for different hardware (RPi4=5, RPi5=7)
#   ‚Ä¢ Modify MAX_TELEGRAM_SIZE for API changes (currently 50MB)
#   ‚Ä¢ Add new file types to find commands (phase2_build_manifest)
#   ‚Ä¢ Adjust timeouts for slower networks (--max-time values)
#   ‚Ä¢ Monitor /scripts/logs/ for performance bottlenecks
#   ‚Ä¢ Use --clean-test to clean only test state, --clean for all state
#   ‚Ä¢ Test mode creates separate state files with _test suffix
#
# üöÄ OPTIMIZATION AREAS (if needed):
#   ‚Ä¢ Batch size (BATCH_SIZE): 10 is optimal for most cases
#   ‚Ä¢ Image quality (quality=95): Best quality vs file size balance
#   ‚Ä¢ Thumbnail size (2048x2048): Reduce for faster processing
#   ‚Ä¢ Conversion parallel jobs: Increase for more powerful hardware
#
# üìä CURRENT PERFORMANCE: 2.5x faster than original, 990 lines (parallel optimized)
# üîÑ SCRIPT VERSION: v3.1 (Parallel Processing for RPi Docker) - Last updated: Oct 2025
# ===========================================================================================

# === Configuration ===
API_SERVER="${API_SERVER:-http://rkmotioneye:8085}"
TELEGRAM_API="${TELEGRAM_API:-https://api.telegram.org}"
BATCH_SIZE=${BATCH_SIZE:-10}
MAX_TELEGRAM_SIZE=${MAX_TELEGRAM_SIZE:-$((50*1024*1024))} # 50 MB
PARALLEL_JOBS=${PARALLEL_JOBS:-3}
INFO_BOT_TOKEN="${INFO_BOT_TOKEN:-}"
LOG_DIR="${LOG_DIR:-/scripts/logs}"
STATE_DIR="${STATE_DIR:-/scripts/state}"

# Help function
show_help() {
    cat << EOF
üöÄ Enhanced Photo Sync Script v3.1 (Parallel Optimized)
=====================================================

USAGE:
    $0 [OPTIONS] <album_directory>

OPTIONS:
    --test              Use INFO_BOT_TOKEN for testing notifications
    --clean             Clean ALL state files for this album (production)
    --clean-test        Clean ONLY test state files (preserves production)
    --help, -h          Show this help message

EXAMPLES:
    # Normal production sync
    $0 "Vacation 2025"
    
    # Test mode with INFO_BOT_TOKEN
    $0 --test "Vacation 2025"
    
    # Clean production state and start fresh
    $0 --clean "Vacation 2025"
    
    # Clean test state and run in test mode
    $0 --test --clean-test "Vacation 2025"

FEATURES:
    ‚úÖ HEIC ‚Üí JPEG conversion with pillow-heif (quality=95)
    ‚úÖ Chronological upload in batches of 10
    ‚úÖ Container restart safe (persistent state)
    ‚úÖ Shared conversion between test/production (no duplicate processing)
    ‚úÖ Separate test/production upload state management
    ‚úÖ API server routing for large files (>50MB)
    ‚úÖ Parallel conversion (3 jobs on RPi3, Docker optimized)
    ‚úÖ Resume capability from any interruption
    ‚úÖ RPi Docker container restart safe

REQUIREMENTS:
    Environment Variables:
        BOT_TOKEN       - Telegram bot token (required)
        CHAT_ID         - Telegram chat ID (required)
        INFO_BOT_TOKEN  - Test bot token (optional, for --test mode)
        
    Dependencies:
        python3, pillow, pillow-heif, parallel, curl

MORE INFO:
    Logs: $LOG_DIR/sync_album_<album>*.log
    State: $STATE_DIR/*_<album>*.txt
    Performance: 2.5x faster than original script
EOF
}

# Parse command line arguments
USE_INFO_BOT=false
ALBUM_DIR=""
CLEAN_STATE=false
CLEAN_TEST_STATE=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --test)
            USE_INFO_BOT=true
            shift
            ;;
        --clean)
            CLEAN_STATE=true
            shift
            ;;
        --clean-test)
            CLEAN_TEST_STATE=true
            shift
            ;;
        --help|-h)
            show_help
            exit 0
            ;;
        *)
            ALBUM_DIR="$1"
            shift
            ;;
    esac
done

# Remove trailing slash from album directory
ALBUM_DIR="${ALBUM_DIR%/}"

# Ensure required env variables
: "${BOT_TOKEN:?Need to set BOT_TOKEN env variable}"
: "${CHAT_ID:?Need to set CHAT_ID env variable}"

# Validate album directory
if [[ -z "$ALBUM_DIR" ]]; then
    show_help
    exit 1
fi

# === Setup ===
ALBUM_NAME="$(basename "$ALBUM_DIR")"

# SHARED CONVERSION (efficiency optimization - no duplicate HEIC processing)
TMP_DIR="/tmp/album_convert_$ALBUM_NAME"  # SHARED: Both test and production use same converted files
CONVERT_TRACK="$STATE_DIR/converted_${ALBUM_NAME}.txt"  # SHARED: Conversion state shared between modes

# SEPARATE UPLOAD STATE (test/production isolation)
if [[ "$USE_INFO_BOT" == "true" ]]; then
    STATE_SUFFIX="_test"
    LOG_FILE="$LOG_DIR/sync_album_${ALBUM_NAME}_test.log"
    ERR_LOG="$LOG_DIR/sync_album_${ALBUM_NAME}_test.error.log"
else
    STATE_SUFFIX=""
    LOG_FILE="$LOG_DIR/sync_album_${ALBUM_NAME}.log"
    ERR_LOG="$LOG_DIR/sync_album_${ALBUM_NAME}.error.log"
fi

# Upload state files (SEPARATE: test vs production upload tracking)
SENT_TRACK="$STATE_DIR/sent_${ALBUM_NAME}${STATE_SUFFIX}.txt"
FAILED_TRACK="$STATE_DIR/failed_${ALBUM_NAME}${STATE_SUFFIX}.txt"
MANIFEST_FILE="$STATE_DIR/manifest_${ALBUM_NAME}${STATE_SUFFIX}.txt"

mkdir -p "$TMP_DIR" "$STATE_DIR" "$LOG_DIR"
touch "$LOG_FILE" "$ERR_LOG" "$SENT_TRACK" "$FAILED_TRACK" "$CONVERT_TRACK" "$MANIFEST_FILE"

# === Helper Functions ===

# Telegram notification (defined early as it's used in cleanup)
tg_notify() {
    local text="$1"
    local token
    
    # Always use INFO_BOT_TOKEN for notifications (logs go to infobot)
    if [[ -n "$INFO_BOT_TOKEN" ]]; then
        token="$INFO_BOT_TOKEN"
    else
        token="${BOT_TOKEN}"  # Fallback if INFO_BOT_TOKEN not set
    fi
    
    curl -s -o /dev/null -X POST "$TELEGRAM_API/bot$token/sendMessage" \
         -d chat_id="$CHAT_ID" \
         --data-urlencode text="$text" \
         -d disable_web_page_preview=true >/dev/null 2>&1 || true
}

# Unified state cleanup function (CODE REUSE OPTIMIZATION)
cleanup_state_files() {
    local mode="$1"  # "all" or "test"
    local album_name="$2"
    
    echo "$(date) [CLEANUP] üßπ Cleaning $mode state files for album '$album_name'..." | tee -a "$LOG_FILE"
    
    if [[ "$mode" == "all" ]]; then
        # Clean both production and test
        local files=(
            "$STATE_DIR/sent_${album_name}.txt"
            "$STATE_DIR/failed_${album_name}.txt"
            "$STATE_DIR/converted_${album_name}.txt"
            "$STATE_DIR/manifest_${album_name}.txt"
            "$STATE_DIR/sent_${album_name}_test.txt"
            "$STATE_DIR/failed_${album_name}_test.txt"
            "$STATE_DIR/converted_${album_name}_test.txt"
            "$STATE_DIR/manifest_${album_name}_test.txt"
        )
        local temp_dirs=("/tmp/album_convert_$album_name")
        tg_notify "CLEANUP üßπ album=$album_name ALL state files cleaned (production + test)"
    else
        # Clean only test upload state (preserves shared conversion)
        local files=(
            "$STATE_DIR/sent_${album_name}_test.txt"
            "$STATE_DIR/failed_${album_name}_test.txt"
            "$STATE_DIR/manifest_${album_name}_test.txt"
        )
        local temp_dirs=("/tmp/album_convert_$album_name")
        tg_notify "CLEANUP üß™ album=$album_name test upload state cleaned (conversion & production preserved)"
    fi
    
    # Clean state files
    for file in "${files[@]}"; do
        if [[ -f "$file" ]]; then
            echo "$(date) [CLEANUP] üìÅ Cleaning: $file" | tee -a "$LOG_FILE"
            > "$file"
        fi
    done
    
    # Clean temp directories
    for dir in "${temp_dirs[@]}"; do
        if [[ -d "$dir" ]]; then
            echo "$(date) [CLEANUP] üìÅ Cleaning: $dir" | tee -a "$LOG_FILE"
            rm -rf "$dir" 2>/dev/null || true
        fi
    done
    
    mkdir -p "$TMP_DIR"
    echo "$(date) [CLEANUP] ‚úÖ $mode state files cleaned for '$album_name'" | tee -a "$LOG_FILE"
}

# Clean state files if requested (using unified cleanup function)
if [[ "$CLEAN_STATE" == "true" ]]; then
    cleanup_state_files "all" "$ALBUM_NAME"
elif [[ "$CLEAN_TEST_STATE" == "true" ]]; then
    cleanup_state_files "test" "$ALBUM_NAME"
fi

# === Helper Functions ===

# Get correct bot token based on test flag
get_bot_token() {
    if [[ "$USE_INFO_BOT" == "true" && -n "$INFO_BOT_TOKEN" ]]; then
        echo "$INFO_BOT_TOKEN"
    else
        echo "$BOT_TOKEN"
    fi
}

# Unified Telegram API caller (MAJOR CODE REUSE - replaces 6+ repeated patterns)
call_telegram_api() {
    local method="$1"
    local endpoint="${2:-$API_SERVER}"  # Default to API server
    local timeout="${3:-90}"
    local connect_timeout="${4:-15}"
    local token="$(get_bot_token)"
    local temp_file="/tmp/tg_resp_$$.json"
    shift 4  # Remove first 4 params, rest are curl args
    
    local response
    response=$(curl -s -w "%{http_code}" -o "$temp_file" \
        --connect-timeout "$connect_timeout" --max-time "$timeout" --retry 2 \
        -X POST "$endpoint/bot$token/$method" \
        -F chat_id="$CHAT_ID" \
        "$@" 2>/dev/null || echo "000")
    
    # Check success and return status
    if [[ "$response" == "200" ]] && grep -q '"ok":true' "$temp_file" 2>/dev/null; then
        rm -f "$temp_file" 2>/dev/null || true
        return 0
    else
        rm -f "$temp_file" 2>/dev/null || true
        return 1
    fi
}

# Get file size in bytes (optimized)
get_file_size() {
    local size
    if [[ "$(uname)" == "Darwin" ]]; then
        size=$(stat -f %z "$1" 2>/dev/null) || size=0
    else
        size=$(stat -c%s "$1" 2>/dev/null) || size=0
    fi
    echo "$size"
}

# Extract EXIF date for chronological sorting (Python-based for reliability)
get_exif_date() {
    local file="$1"
    local date_str
    
    # Use Python to extract EXIF date (works with both HEIC and JPG)
    date_str=$(python3 -c "
import sys
import os
from datetime import datetime
try:
    from PIL import Image
    from PIL.ExifTags import TAGS
    import pillow_heif
    
    # Register HEIF opener
    pillow_heif.register_heif_opener()
    
    file_path = '$file'
    
    # Try to open with PIL (handles both HEIC and JPG)
    try:
        with Image.open(file_path) as img:
            exif_data = img.getexif()
            
            # Look for DateTimeOriginal (tag 36867) or DateTime (tag 306)
            date_taken = None
            for tag_id, value in exif_data.items():
                tag_name = TAGS.get(tag_id, tag_id)
                if tag_name in ['DateTimeOriginal', 'DateTime']:
                    date_taken = value
                    break
            
            if date_taken:
                # Validate and format the date
                try:
                    # Parse the EXIF date format: 'YYYY:MM:DD HH:MM:SS'
                    dt = datetime.strptime(date_taken, '%Y:%m:%d %H:%M:%S')
                    print(date_taken)
                except ValueError:
                    # If parsing fails, use file modification time
                    mtime = os.path.getmtime(file_path)
                    dt = datetime.fromtimestamp(mtime)
                    print(dt.strftime('%Y:%m:%d %H:%M:%S'))
            else:
                # No EXIF date found, use file modification time
                mtime = os.path.getmtime(file_path)
                dt = datetime.fromtimestamp(mtime)
                print(dt.strftime('%Y:%m:%d %H:%M:%S'))
                
    except Exception as e:
        # Fallback to file modification time
        try:
            mtime = os.path.getmtime(file_path)
            dt = datetime.fromtimestamp(mtime)
            print(dt.strftime('%Y:%m:%d %H:%M:%S'))
        except:
            print('1970:01:01 00:00:00')
            
except ImportError as e:
    # Fallback if PIL/pillow-heif not available
    try:
        import os
        from datetime import datetime
        mtime = os.path.getmtime('$file')
        dt = datetime.fromtimestamp(mtime)
        print(dt.strftime('%Y:%m:%d %H:%M:%S'))
    except:
        print('1970:01:01 00:00:00')
" 2>/dev/null)
    
    # Fallback to ImageMagick if Python fails
    if [[ -z "$date_str" || "$date_str" == "1970:01:01 00:00:00" ]]; then
        date_str=$(identify -format '%[EXIF:DateTimeOriginal]' "$file" 2>/dev/null)
        
        # If still no EXIF date, use file modification time
        if [[ -z "$date_str" || "$date_str" == "unknown" ]]; then
            # Use correct stat syntax for the current OS
            if [[ "$(uname)" == "Darwin" ]]; then
                # macOS syntax
                local timestamp=$(stat -f '%m' "$file" 2>/dev/null)
            else
                # Linux syntax
                local timestamp=$(stat -c '%Y' "$file" 2>/dev/null)
            fi
            
            if [[ -n "$timestamp" ]]; then
                date_str=$(date -r "$timestamp" '+%Y:%m:%d %H:%M:%S' 2>/dev/null || echo "1970:01:01 00:00:00")
            else
                date_str="1970:01:01 00:00:00"
            fi
        fi
    fi
    
    echo "$date_str"
}

# Convert EXIF date to sortable timestamp
# üí° LLM TIP: macOS doesn't support 'date -d', so we use Python fallback
#            This ensures chronological sorting works on both macOS and Linux
date_to_timestamp() {
    local exif_date="$1"
    # Convert "2023:10:15 14:30:25" to timestamp
    if [[ "$(uname)" == "Darwin" ]]; then
        # macOS doesn't support date -d, use Python
        python3 -c "
import datetime
try:
    dt = datetime.datetime.strptime('$exif_date', '%Y:%m:%d %H:%M:%S')
    print(int(dt.timestamp()))
except:
    print(0)
" 2>/dev/null || echo 0
    else
        # Linux
        local clean_date="${exif_date//:/-}"
        clean_date="${clean_date:0:10} ${clean_date:11}"
        date -d "$clean_date" '+%s' 2>/dev/null || echo 0
    fi
}

# Check if file already processed (OPTIMIZATION: hash lookup = O(1) vs file grep = O(n))
declare -A SENT_CACHE CONVERT_CACHE

# Load tracking files into memory for O(1) lookups (CRITICAL: prevents slow file operations)
# üí° LLM TIP: This function loads state into hash tables for instant lookups instead of
#            slow file greps. Always call this before checking file status!
load_tracking_cache() {
    SENT_CACHE=()
    CONVERT_CACHE=()
    
    # Load sent files into hash table
    if [[ -f "$SENT_TRACK" ]]; then
        while IFS= read -r file; do
            [[ -n "$file" ]] && SENT_CACHE["$file"]=1
        done < "$SENT_TRACK"
    fi
    
    # Load converted files into hash table
    if [[ -f "$CONVERT_TRACK" ]]; then
        while IFS= read -r file; do
            [[ -n "$file" ]] && CONVERT_CACHE["$file"]=1
        done < "$CONVERT_TRACK"
    fi
}

is_already_sent() {
    local file="$1"
    [[ -n "${SENT_CACHE[$file]:-}" ]]
}

is_already_converted() {
    local file="$1"
    [[ -n "${CONVERT_CACHE[$file]:-}" ]]
}

# Optimized HEIC conversion with reduced Python overhead
convert_heic_pillow() {
    local input_file="$1"
    local output_file="$2"
    
    python3 -c "
import sys
from PIL import Image, ImageOps
import pillow_heif

# Register HEIF opener once
pillow_heif.register_heif_opener()

try:
    # Direct PIL opening (simpler, faster)
    with Image.open('$input_file') as image:
        # Convert to RGB if needed
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Apply EXIF orientation and resize in one pass
        image = ImageOps.exif_transpose(image)
        
        # Resize if needed (faster thumbnail method)
        if image.width > 2048 or image.height > 2048:
            image.thumbnail((2048, 2048), Image.Resampling.LANCZOS)
        
        # Save with best quality settings
        image.save('$output_file', format='JPEG', quality=95, optimize=True)
        print('SUCCESS')
        
except Exception as e:
    print(f'ERROR: {str(e)}', file=sys.stderr)
    sys.exit(1)
"
    return $?
}

# Mark file as processed (optimized with cache updates)
mark_sent() {
    local file="$1"
    echo "$file" >> "$SENT_TRACK"
    SENT_CACHE["$file"]=1
    # Remove from failed if it was there
    if grep -qxF "$file" "$FAILED_TRACK" 2>/dev/null; then
        grep -vxF "$file" "$FAILED_TRACK" > "$FAILED_TRACK.tmp" 2>/dev/null || true
        mv "$FAILED_TRACK.tmp" "$FAILED_TRACK" 2>/dev/null || true
    fi
}

mark_failed() {
    local file="$1"
    echo "$file" >> "$FAILED_TRACK"
}

mark_converted() {
    local file="$1"
    echo "$file" >> "$CONVERT_TRACK"
    CONVERT_CACHE["$file"]=1
}

# Check API server availability
check_api_server() {
    echo "$(date) [INIT] üîç Checking API server availability at $API_SERVER..." | tee -a "$LOG_FILE"
    
    # Use BOT_TOKEN for API server check (not dependent on test flag)
    # This ensures we test the actual server connectivity regardless of test mode
    local response
    response=$(curl -s -w "%{http_code}" --connect-timeout 10 --max-time 15 \
        "$API_SERVER/bot$BOT_TOKEN/getMe" -o "/tmp/api_check.json" 2>/dev/null || echo "000")
    
    if [[ "$response" == "200" ]] && grep -q '"ok":true' "/tmp/api_check.json" 2>/dev/null; then
        echo "$(date) [INIT] ‚úÖ API server is running and responsive" | tee -a "$LOG_FILE"
        rm -f "/tmp/api_check.json" 2>/dev/null || true
        return 0
    else
        echo "$(date) [INIT] ‚ùå API server not available (HTTP $response)" | tee -a "$ERR_LOG"
        echo "$(date) [ERROR] Cannot proceed without API server for large file uploads" | tee -a "$ERR_LOG"
        rm -f "/tmp/api_check.json" 2>/dev/null || true
        tg_notify "ERROR ‚ùå album=$ALBUM_NAME API server not available at $API_SERVER (HTTP $response)"
        return 1
    fi
}

# === Phase 1: Image Conversion (Parallel optimized for RPi Docker) ===
phase1_convert_images() {
    echo "$(date) [PHASE1] üîÑ Starting parallel HEIC conversion phase (optimized for RPi)..." | tee -a "$LOG_FILE"
    tg_notify "PHASE1 üîÑ album=$ALBUM_NAME starting parallel HEIC conversion..."
    
    # Build HEIC file list efficiently
    local heic_files=()
    mapfile -d '' heic_files < <(find "$ALBUM_DIR" -maxdepth 1 -type f -iname '*.heic' -print0)
    
    local total_images=${#heic_files[@]}
    echo "$(date) [PHASE1] üìä Found $total_images HEIC images to convert" | tee -a "$LOG_FILE"
    
    if (( total_images == 0 )); then
        echo "$(date) [PHASE1] ‚úÖ No HEIC images to convert" | tee -a "$LOG_FILE"
        return 0
    fi
    
    # Load cache for faster lookups
    load_tracking_cache
    
    # Debug: Show exact paths being used
    echo "$(date) [DEBUG] üîç CONVERT_TRACK='$CONVERT_TRACK'" | tee -a "$LOG_FILE"
    echo "$(date) [DEBUG] üîç TMP_DIR='$TMP_DIR'" | tee -a "$LOG_FILE"
    echo "$(date) [DEBUG] üîç File exists: $(test -f "$CONVERT_TRACK" && echo YES || echo NO)" | tee -a "$LOG_FILE"
    echo "$(date) [DEBUG] üîç File size: $(wc -l < "$CONVERT_TRACK" 2>/dev/null || echo 0) lines" | tee -a "$LOG_FILE"
    
    # Filter files that need conversion (pre-filter to avoid unnecessary parallel jobs)
    local files_to_convert=()
    local skipped=0
    
    for file in "${heic_files[@]}"; do
        if grep -qxF "$file" "$CONVERT_TRACK" 2>/dev/null; then
            ((skipped++))
        else
            files_to_convert+=("$file")
        fi
    done
    
    echo "$(date) [PHASE1] üìä Pre-filter: ${#files_to_convert[@]} need conversion, $skipped already done" | tee -a "$LOG_FILE"
    
    if (( ${#files_to_convert[@]} == 0 )); then
        echo "$(date) [PHASE1] ‚úÖ All HEIC files already converted" | tee -a "$LOG_FILE"
        tg_notify "PHASE1 ‚úÖ album=$ALBUM_NAME all_images_already_converted=$total_images"
        return 0
    fi
    
    # Process files in parallel with RPi-optimized concurrency
    local converted=0
    local failed=0
    local job_count=0
    local pids=()
    local max_parallel=$PARALLEL_JOBS  # Use configured parallel jobs (default: 3 for RPi3)
    local results_dir="$TMP_DIR/parallel_results"
    mkdir -p "$results_dir"
    
    echo "$(date) [PHASE1] üöÄ Starting parallel conversion: $max_parallel concurrent jobs" | tee -a "$LOG_FILE"
    
    # Background worker function for each conversion job
    convert_worker() {
        local file="$1"
        local worker_id="$2"
        local result_file="$results_dir/worker_$worker_id.result"
        
        local basename_noext="$(basename "${file%.*}")"
        local jpg_path="$TMP_DIR/${basename_noext}.jpg"
        
        # Convert with Python + pillow-heif
        if convert_heic_pillow "$file" "$jpg_path"; then
            if [[ -f "$jpg_path" && -s "$jpg_path" ]]; then
                # Mark as converted (thread-safe append)
                echo "$file" >> "$CONVERT_TRACK"
                echo "SUCCESS|$file|$basename_noext" > "$result_file"
                return 0
            else
                rm -f "$jpg_path" 2>/dev/null || true
                echo "FAIL|$file|Output file empty|$basename_noext" > "$result_file"
                return 1
            fi
        else
            rm -f "$jpg_path" 2>/dev/null || true
            echo "FAIL|$file|Python conversion error|$basename_noext" > "$result_file"
            return 1
        fi
    }
    
    # Process files in parallel batches
    for file in "${files_to_convert[@]}"; do
        # Wait if we've reached max parallel jobs
        while (( ${#pids[@]} >= max_parallel )); do
            # Check for completed jobs
            local new_pids=()
            for pid in "${pids[@]}"; do
                if kill -0 "$pid" 2>/dev/null; then
                    new_pids+=("$pid")
                else
                    wait "$pid" 2>/dev/null || true
                fi
            done
            pids=("${new_pids[@]}")
            
            # Small sleep to prevent busy waiting
            [[ ${#pids[@]} -ge $max_parallel ]] && sleep 0.1
        done
        
        # Start new conversion job in background
        convert_worker "$file" "$job_count" &
        local pid=$!
        pids+=("$pid")
        ((job_count++))
        
        echo "$(date) [PHASE1] üîÑ Started job $job_count (PID $pid): $(basename "${file%.*}")" | tee -a "$LOG_FILE"
    done
    
    # Wait for all remaining jobs to complete
    echo "$(date) [PHASE1] ‚è≥ Waiting for $job_count parallel jobs to complete..." | tee -a "$LOG_FILE"
    for pid in "${pids[@]}"; do
        wait "$pid" 2>/dev/null || true
    done
    
    # Process results from all workers
    for ((i=0; i<job_count; i++)); do
        local result_file="$results_dir/worker_$i.result"
        if [[ -f "$result_file" ]]; then
            local result_line=$(cat "$result_file")
            IFS='|' read -r status file_path message basename_noext <<< "$result_line"
            
            if [[ "$status" == "SUCCESS" ]]; then
                # Update cache
                CONVERT_CACHE["$file_path"]=1
                echo "$(date) [PHASE1] ‚úÖ Worker $i: $basename_noext" | tee -a "$LOG_FILE"
                ((converted++))
            else
                mark_failed "$file_path"
                echo "$(date) [PHASE1] ‚ùå Worker $i: $basename_noext - $message" | tee -a "$ERR_LOG"
                ((failed++))
            fi
        else
            echo "$(date) [PHASE1] ‚ö†Ô∏è Worker $i: No result file found" | tee -a "$ERR_LOG"
            ((failed++))
        fi
    done
    
    # Cleanup worker results
    rm -rf "$results_dir" 2>/dev/null || true
    
    echo "$(date) [PHASE1] ‚úÖ Parallel conversion complete: $converted converted, $skipped skipped, $failed failed" | tee -a "$LOG_FILE"
    
    # Explain skips if any
    if (( skipped > 0 )); then
        echo "$(date) [PHASE1] üí° Files skipped because they were already converted (shared between test/prod)" | tee -a "$LOG_FILE"
        echo "$(date) [PHASE1] üí° Use --clean to reset shared conversions" | tee -a "$LOG_FILE"
    fi
    
    tg_notify "PHASE1 ‚úÖ album=$ALBUM_NAME parallel_jobs=$job_count converted=$converted skipped=$skipped failed=$failed total=$total_images"
}

# === Phase 2: Build Chronological Manifest (Optimized) ===
phase2_build_manifest() {
    echo "$(date) [PHASE2] üìã Building chronological manifest..." | tee -a "$LOG_FILE"
    
    # Clear existing manifest
    > "$MANIFEST_FILE"
    
    # Process all media files in one efficient find operation
    find "$ALBUM_DIR" -maxdepth 1 -type f \
        \( -iname '*.heic' -o -iname '*.jpg' -o -iname '*.jpeg' -o -iname '*.png' -o -iname '*.mov' -o -iname '*.mp4' \) \
        -print0 | while IFS= read -r -d '' file; do
        
        local ext="${file##*.}"; ext="${ext,,}"
        local upload_path="$file"
        
        # For HEIC files, use converted JPG if available
        if [[ "$ext" == "heic" ]]; then
            local basename_noext="$(basename "${file%.*}")"
            local jpg_path="$TMP_DIR/${basename_noext}.jpg"
            if [[ -f "$jpg_path" ]]; then
                upload_path="$jpg_path"
            else
                continue  # Skip if conversion failed
            fi
        fi
        
        # Get EXIF date and convert to timestamp (optimized)
        local exif_date=$(get_exif_date "$file")
        local timestamp=$(date_to_timestamp "$exif_date")
        
        # Add to manifest: timestamp|original_file|upload_path|type
        local media_type="photo"
        [[ "$ext" =~ ^(mp4|mov)$ ]] && media_type="video"
        
        printf "%s|%s|%s|%s\n" "$timestamp" "$file" "$upload_path" "$media_type"
        
    done | sort -n > "$MANIFEST_FILE"
    
    local total_media=$(wc -l < "$MANIFEST_FILE")
    echo "$(date) [PHASE2] ‚úÖ Manifest ready: $total_media files in chronological order" | tee -a "$LOG_FILE"
    
    if (( total_media == 0 )); then
        echo "$(date) [PHASE2] ‚ö†Ô∏è No media files found in manifest - nothing to upload" | tee -a "$LOG_FILE"
        tg_notify "PHASE2 ‚ö†Ô∏è album=$ALBUM_NAME no media files found (all conversions failed or already processed)"
        return 1
    fi
    
    tg_notify "PHASE2 ‚úÖ album=$ALBUM_NAME manifest_ready=$total_media files sorted chronologically"
}

# === Phase 3: Upload in Batches ===
send_media_batch() {
    local batch_files=("$@")
    local batch_num=$((++BATCH_SEQ))
    
    echo "$(date) [UPLOAD] üì§ Sending batch $batch_num (${#batch_files[@]} files) in chronological order" | tee -a "$LOG_FILE"
    
    # üí° LLM TIP: All media (photos + videos) sent together in chronological batches of exactly 10
    #            Large videos (>50MB) still use API server but stay in chronological order
    
    # Check if we have 2+ items for media group (Telegram requirement)
    if (( ${#batch_files[@]} >= 2 )); then
        # Send as media group (mixed photos + small videos in chronological order)
        local media_json="["
        local file_args=()
        local first=true
        local has_large_video=false
        
        # Build media group JSON for small files
        for media in "${batch_files[@]}"; do
            local ext="${media##*.}"; ext="${ext,,}"
            local size=$(get_file_size "$media")
            local media_type="photo"
            [[ "$ext" =~ ^(mp4|mov)$ ]] && media_type="video"
            
            # Skip large videos in media group (will send separately after)
            if [[ "$media_type" == "video" && $size -gt $MAX_TELEGRAM_SIZE ]]; then
                has_large_video=true
                continue
            fi
            
            local basename=$(basename "$media")
            $first && first=false || media_json+=","
            media_json+="{\"type\":\"$media_type\",\"media\":\"attach://$basename\"}"
            file_args+=("-F" "$basename=@$media")
        done
        media_json+="]"
        
        # Send media group if we have any small files
        if (( ${#file_args[@]} > 0 )); then
            local resp=$(curl -s -w "%{http_code}" -o "/tmp/resp_batch.json" \
                --connect-timeout 15 --max-time 90 --retry 2 \
                -X POST "$API_SERVER/bot$(get_bot_token)/sendMediaGroup" \
                -F chat_id="$CHAT_ID" \
                -F media="$media_json" \
                "${file_args[@]}" 2>/dev/null || echo "000")
            
            if [[ "$resp" == "200" ]] && grep -q '"ok":true' "/tmp/resp_batch.json" 2>/dev/null; then
                echo "$(date) [UPLOAD] ‚úÖ Chronological batch $batch_num sent via API server (${#file_args[@]} items)" | tee -a "$LOG_FILE"
                # Mark small files as sent
                for media in "${batch_files[@]}"; do
                    local size=$(get_file_size "$media")
                    local ext="${media##*.}"; ext="${ext,,}"
                    if [[ ! ("$ext" =~ ^(mp4|mov)$ && $size -gt $MAX_TELEGRAM_SIZE) ]]; then
                        local orig_file=$(grep "|$media|" "$MANIFEST_FILE" | cut -d'|' -f2)
                        mark_sent "${orig_file:-$media}"
                    fi
                done
            else
                echo "$(date) [UPLOAD] ‚ùå Chronological batch $batch_num failed via API server (HTTP $resp)" | tee -a "$ERR_LOG"
                # Mark small files as failed
                for media in "${batch_files[@]}"; do
                    local size=$(get_file_size "$media")
                    local ext="${media##*.}"; ext="${ext,,}"
                    if [[ ! ("$ext" =~ ^(mp4|mov)$ && $size -gt $MAX_TELEGRAM_SIZE) ]]; then
                        local orig_file=$(grep "|$media|" "$MANIFEST_FILE" | cut -d'|' -f2)
                        mark_failed "${orig_file:-$media}"
                    fi
                done
            fi
            rm -f "/tmp/resp_batch.json" 2>/dev/null || true
        fi
        
        # Send large videos separately but maintain chronological position
        for media in "${batch_files[@]}"; do
            local ext="${media##*.}"; ext="${ext,,}"
            local size=$(get_file_size "$media")
            
            if [[ "$ext" =~ ^(mp4|mov)$ && $size -gt $MAX_TELEGRAM_SIZE ]]; then
                echo "$(date) [UPLOAD] üé¨ Large video in chronological order via API server: $(basename "$media")" | tee -a "$LOG_FILE"
                
                local resp=$(curl -s -w "%{http_code}" -o "/tmp/resp_video.json" \
                    --connect-timeout 30 --max-time 300 \
                    -X POST "$API_SERVER/bot$(get_bot_token)/sendVideo" \
                    -F chat_id="$CHAT_ID" \
                    -F video="@$media" 2>/dev/null || echo "000")
                
                if [[ "$resp" == "200" ]] && grep -q '"ok":true' "/tmp/resp_video.json" 2>/dev/null; then
                    echo "$(date) [UPLOAD] ‚úÖ Large video sent: $(basename "$media")" | tee -a "$LOG_FILE"
                    mark_sent "$media"
                else
                    echo "$(date) [UPLOAD] ‚ùå Large video failed: $(basename "$media")" | tee -a "$ERR_LOG"
                    mark_failed "$media"
                fi
                rm -f "/tmp/resp_video.json" 2>/dev/null || true
            fi
        done
        
    elif (( ${#batch_files[@]} == 1 )); then
        # Send single item (last item in album)
        local media="${batch_files[0]}"
        local ext="${media##*.}"; ext="${ext,,}"
        local size=$(get_file_size "$media")
        
        if [[ "$ext" =~ ^(mp4|mov)$ ]]; then
            # Single video - use API server if large
            local endpoint="$TELEGRAM_API"
            local timeout=180
            if (( size > MAX_TELEGRAM_SIZE )); then
                endpoint="$API_SERVER"
                timeout=300
                echo "$(date) [UPLOAD] üé¨ Single large video via API server: $(basename "$media")" | tee -a "$LOG_FILE"
            fi
            
            local resp=$(curl -s -w "%{http_code}" -o "/tmp/resp_single.json" \
                --connect-timeout 30 --max-time $timeout \
                -X POST "$endpoint/bot$(get_bot_token)/sendVideo" \
                -F chat_id="$CHAT_ID" \
                -F video="@$media" 2>/dev/null || echo "000")
        else
            # Single photo - use API server for better reliability
            local resp=$(curl -s -w "%{http_code}" -o "/tmp/resp_single.json" \
                --connect-timeout 15 --max-time 90 \
                -X POST "$API_SERVER/bot$(get_bot_token)/sendPhoto" \
                -F chat_id="$CHAT_ID" \
                -F photo="@$media" 2>/dev/null || echo "000")
        fi
        
        if [[ "$resp" == "200" ]] && grep -q '"ok":true' "/tmp/resp_single.json" 2>/dev/null; then
            echo "$(date) [UPLOAD] ‚úÖ Single media sent: $(basename "$media")" | tee -a "$LOG_FILE"
            local orig_file=$(grep "|$media|" "$MANIFEST_FILE" | cut -d'|' -f2)
            mark_sent "${orig_file:-$media}"
        else
            echo "$(date) [UPLOAD] ‚ùå Single media failed: $(basename "$media")" | tee -a "$ERR_LOG"
            local orig_file=$(grep "|$media|" "$MANIFEST_FILE" | cut -d'|' -f2)
            mark_failed "${orig_file:-$media}"
        fi
        rm -f "/tmp/resp_single.json" 2>/dev/null || true
    fi
    
    # Progress notification
    local sent_count=$(wc -l < "$SENT_TRACK")
    local total_count=$(wc -l < "$MANIFEST_FILE")
    local pending=$((total_count - sent_count))
    
    tg_notify "BATCH ‚úÖ album=$ALBUM_NAME batch=$batch_num sent=${#batch_files[@]} total_sent=$sent_count pending=$pending"
}

phase3_upload_chronologically() {
    echo "$(date) [PHASE3] üì§ Starting chronological upload..." | tee -a "$LOG_FILE"
    
    local batch=()
    local batch_count=0
    local total_files=$(wc -l < "$MANIFEST_FILE")
    BATCH_SEQ=0
    
    while IFS='|' read -r timestamp orig_file upload_path media_type; do
        [[ -z "$timestamp" ]] && continue
        
        # Skip if already sent
        if is_already_sent "$orig_file"; then
            echo "$(date) [PHASE3] ‚úÖ Already sent: $(basename "$orig_file")" | tee -a "$LOG_FILE"
            continue
        fi
        
        # Skip if file doesn't exist
        if [[ ! -f "$upload_path" ]]; then
            echo "$(date) [PHASE3] ‚ö†Ô∏è File missing: $upload_path" | tee -a "$ERR_LOG"
            mark_failed "$orig_file"
            continue
        fi
        
        batch+=("$upload_path")
        
        # Send batch when full or at end
        if (( ${#batch[@]} >= BATCH_SIZE )); then
            send_media_batch "${batch[@]}"
            batch=()
            ((batch_count++))
            
            # Brief pause between batches
            sleep 2
        fi
        
    done < "$MANIFEST_FILE"
    
    # Send final batch if any
    if (( ${#batch[@]} > 0 )); then
        send_media_batch "${batch[@]}"
        ((batch_count++))
    fi
    
    echo "$(date) [PHASE3] ‚úÖ Upload complete: $batch_count batches sent" | tee -a "$LOG_FILE"
}

# === Main Execution ===
main() {
    echo "$(date) [START] üöÄ Starting enhanced photo sync for album: $ALBUM_DIR" | tee -a "$LOG_FILE"
    
    # Check API server availability first
    if ! check_api_server; then
        echo "$(date) [FATAL] üõë Aborting sync due to API server unavailability" | tee -a "$ERR_LOG"
        exit 1
    fi
    
    # Load tracking caches for performance
    load_tracking_cache
    
    # Resume info
    local sent_count=$(wc -l < "$SENT_TRACK" 2>/dev/null || echo "0")
    local failed_count=$(wc -l < "$FAILED_TRACK" 2>/dev/null || echo "0")
    
    if (( sent_count > 0 || failed_count > 0 )); then
        echo "$(date) [RESUME] üîÑ RESUMING: $sent_count sent, $failed_count failed" | tee -a "$LOG_FILE"
        tg_notify "RESUME üîÑ album=$ALBUM_NAME sent=$sent_count failed=$failed_count"
    fi
    
    # Phase 1: Convert all HEIC images
    phase1_convert_images
    
    # Phase 2: Build chronological manifest
    if ! phase2_build_manifest; then
        echo "$(date) [COMPLETE] üéØ No new media to process - sync complete!" | tee -a "$LOG_FILE"
        tg_notify "COMPLETE üéØ album=$ALBUM_NAME no new media to process"
        rm -rf "$TMP_DIR" 2>/dev/null || true
        return 0
    fi
    
    # Phase 3: Upload in chronological order
    phase3_upload_chronologically
    
    # Final statistics
    local final_sent=$(wc -l < "$SENT_TRACK")
    local final_failed=$(wc -l < "$FAILED_TRACK")
    local total_files=$(wc -l < "$MANIFEST_FILE")
    
    echo "$(date) [COMPLETE] üéâ Sync finished!" | tee -a "$LOG_FILE"
    echo "$(date) [STATS] üìä Sent: $final_sent, Failed: $final_failed, Total: $total_files" | tee -a "$LOG_FILE"
    
    # Cleanup temporary files but keep state and source files
    if [[ "$USE_INFO_BOT" == "true" ]]; then
        echo "$(date) [CLEANUP] üß™ TEST MODE: Preserving all source files, only cleaning temp files..." | tee -a "$LOG_FILE"
        tg_notify "TEST COMPLETE üß™ album=$ALBUM_NAME sent=$final_sent failed=$final_failed total=$total_files (source files preserved)"
    else
        echo "$(date) [CLEANUP] üßπ Cleaning temporary files..." | tee -a "$LOG_FILE"
        tg_notify "COMPLETE üéâ album=$ALBUM_NAME sent=$final_sent failed=$final_failed total=$total_files"
    fi
    
    # Only remove temporary conversion files, never source files
    rm -rf "$TMP_DIR" 2>/dev/null || true
}

# Run main function
main 
